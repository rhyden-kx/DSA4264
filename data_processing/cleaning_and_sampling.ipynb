{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSA4264 Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import modules\n",
    "import emoji\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the data (STORE FILES IN SUBFOLDER CALLED DATA)\n",
    "data2020_df = pd.read_csv('./data/Reddit-Threads_2020-2021.csv',  lineterminator='\\n', encoding='utf8')\n",
    "data2022_df = pd.read_csv('./data/Reddit-Threads_2022-2023.csv', lineterminator='\\n', encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2663782\n",
      "1840541\n"
     ]
    }
   ],
   "source": [
    "#print(len(data2020_df))\n",
    "#print(len(data2022_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data cleaning\n",
    "\n",
    "def clean_data(df):\n",
    "  pd.options.mode.copy_on_write = True\n",
    "\n",
    "  #Remove rows with empty review_text\n",
    "  df = df[df['text'].notnull()]\n",
    "\n",
    "  #Remove emoji rows\n",
    "  df['text'] = df['text'].apply(lambda x: emoji.replace_emoji(x,''))\n",
    "\n",
    "  #Remove punctuation\n",
    "  df['text'] = df['text'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "\n",
    "  #Remove all rows that has non ASCII characters\n",
    "  df = df[df['text'].apply(lambda x: all(ord(c) < 128 for c in x))]\n",
    "\n",
    "  #Set all to lower case\n",
    "  df['text'] = df['text'].str.lower()\n",
    "  \n",
    "  #remove deleted rows\n",
    "  #Remove comments which are [Removed] or [Deleted]\n",
    "  df = df[~df['text'].str.contains(r'\\[removed\\]', na=False)]\n",
    "  df = df[~df['text'].str.contains(r'\\[deleted\\]', na=False)]\n",
    "\n",
    "\n",
    "  '''\n",
    "  #Remove rows that are not in English\n",
    "  count = 1\n",
    "  for index, row in df.iterrows():\n",
    "      try:\n",
    "        if detect(row['text']) != 'en':\n",
    "          df.drop(index, inplace=True)\n",
    "          print(row['text'])\n",
    "          print(count)\n",
    "          count += 1\n",
    "      except:\n",
    "        df.drop(index, inplace=True)\n",
    "        print(\"error for\", row['text'])\n",
    "  '''\n",
    "\n",
    "  df = df.reset_index(drop=True)\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaned_2020 = clean_data(data2020_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaned_2022 = clean_data(data2022_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500000\n"
     ]
    }
   ],
   "source": [
    "#Get random sample of 500000\n",
    "length_2020 = len(data2020_df)\n",
    "length_2022 = len(data2022_df)\n",
    "size2020 = int(500000 * (length_2020/(length_2020 + length_2022)))\n",
    "size2022 = 500000 - size2020\n",
    "\n",
    "sample_2020 = data2020_df.sample(n=size2020, random_state=42)\n",
    "sample_2022 = data2022_df.sample(n=size2022, random_state=42)\n",
    "\n",
    "combined_df = pd.concat([sample_2020, sample_2022], axis=0)\n",
    "combined_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(len(combined_df)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = clean_data(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "298642\n",
      "201358\n"
     ]
    }
   ],
   "source": [
    "#print(size2020)\n",
    "#print(size2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "439642\n",
      "           text            timestamp   username  \\\n",
      "count    439642               439642     439642   \n",
      "unique   382044               438442      38054   \n",
      "top     deleted  2020-04-16 06:15:34  [deleted]   \n",
      "freq      33734                    3      53326   \n",
      "\n",
      "                                                     link    link_id  \\\n",
      "count                                              439642     439642   \n",
      "unique                                             439642      68190   \n",
      "top     /r/singapore/comments/tiw1t8/rsingapore_random...  t3_homxdq   \n",
      "freq                                                    1       1012   \n",
      "\n",
      "        parent_id       id subreddit_id  \\\n",
      "count      439642   439642       439642   \n",
      "unique     332533   439642            3   \n",
      "top     t3_homxdq  i1hghen     t5_2qh8c   \n",
      "freq          486        1       396643   \n",
      "\n",
      "                                               moderation  \n",
      "count                                              439642  \n",
      "unique                                                 44  \n",
      "top     {'removal_reason': None, 'collapsed': False, '...  \n",
      "freq                                               119857  \n"
     ]
    }
   ],
   "source": [
    "print(len(cleaned_df))\n",
    "print(cleaned_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save data to csv (Only need to run once)\n",
    "#cleaned_df.to_csv('Reddit_cleaned_500k.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
