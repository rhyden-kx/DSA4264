{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hatefulness and Toxicity Analysis\n",
    "\n",
    "This notebook is meant to create a pipeline for toxicity and hatefulness analysis, specifically for reddit data. As reddit data tends to be more toxic and hateful than other social media pages, we will focus this notebook on analysing existing Singaporean subreddit data provided in class.\n",
    "\n",
    "The aim of this notebook is to provide a formmat for users to follow to recreate the results we had, and also to follow our methodology of analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rhyde\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rhyde\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rhyde\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\rhyde\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard Library Imports\n",
    "import ast\n",
    "import datetime\n",
    "import html\n",
    "import io\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "import base64\n",
    "import dash\n",
    "import dash_bootstrap_components as dbc\n",
    "from dash import Dash, dcc, html, dash_table, Input, Output, State, callback\n",
    "from dash.dash_table.Format import Group\n",
    "from dash.dependencies import Input, Output, State\n",
    "from dash_bootstrap_templates import load_figure_template\n",
    "\n",
    "# Gensim Imports\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "import gensim.utils as gu\n",
    "import ldamallet\n",
    "\n",
    "# Hugging Face & Transformer Imports\n",
    "from huggingface_hub import hf_hub_download\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Matplotlib Imports\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.ticker import FuncFormatter, MaxNLocator\n",
    "\n",
    "# NLTK Imports\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Plotly Imports\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.graph_objs as go  # Duplicate alias but keeping it here if both are required\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Pandas and Numpy Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Scipy and Statsmodels Imports\n",
    "from scipy.stats import f_oneway  # ANOVA test\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "# Scikit-Learn Imports\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "# Visualization Imports\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Text Analysis Imports\n",
    "import emoji\n",
    "from langdetect import detect\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Torch Imports (for models on local system or device)\n",
    "import torch\n",
    "\n",
    "# ONNX Runtime (for deploying models using ONNX)\n",
    "import onnxruntime as rt\n",
    "\n",
    "# NLTK Downloads\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the data (STORE FILES IN SUBFOLDER CALLED DATA. REname file path accordingly)\n",
    "data2020_df = pd.read_csv('./data/Reddit-Threads_2020-2021.csv',  lineterminator='\\n', encoding='utf8')\n",
    "data2022_df = pd.read_csv('./data/Reddit-Threads_2022-2023.csv', lineterminator='\\n', encoding='utf8')\n",
    "print(len(data2020_df)) #2663782\n",
    "print(len(data2022_df)) #1840541"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "  pd.options.mode.copy_on_write = True\n",
    "\n",
    "  #Remove rows with empty review_text\n",
    "  df = df[df['text'].notnull()]\n",
    "\n",
    "  #Remove emoji rows\n",
    "  df['text'] = df['text'].apply(lambda x: emoji.replace_emoji(x,''))\n",
    "\n",
    "  #Remove punctuation\n",
    "  df['text'] = df['text'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "\n",
    "  #Remove all rows that has non ASCII characters\n",
    "  df = df[df['text'].apply(lambda x: all(ord(c) < 128 for c in x))]\n",
    "\n",
    "  #Set all to lower case\n",
    "  df['text'] = df['text'].str.lower()\n",
    "\n",
    "  df = df.reset_index(drop=True)\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get random sample of 500,000\n",
    "length_2020 = len(data2020_df)\n",
    "length_2022 = len(data2022_df)\n",
    "size2020 = int(500000 * (length_2020/(length_2020 + length_2022)))\n",
    "size2022 = 500000 - size2020\n",
    "\n",
    "sample_2020 = data2020_df.sample(n=size2020, random_state=42)\n",
    "sample_2022 = data2022_df.sample(n=size2022, random_state=42)\n",
    "\n",
    "combined_df = pd.concat([sample_2020, sample_2022], axis=0)\n",
    "combined_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(len(combined_df))  #500,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get a glimpse of the data\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean Data:\n",
    "cleaned_df = clean_data(combined_df)\n",
    "\n",
    "print(len(cleaned_df)) #original 439642\n",
    "print(cleaned_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save data to csv (Only need to run once)\n",
    "cleaned_df.to_csv('Reddit_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df = cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data to appropriate datatypes\n",
    "reddit_df['text'] = reddit_df['text'].astype(str)\n",
    "reddit_df['timestamp'] = pd.to_datetime(reddit_df['timestamp'])\n",
    "reddit_df['username'] = reddit_df['username'].astype(str)\n",
    "reddit_df['link'] = reddit_df['link'].astype(str)\n",
    "reddit_df['link_id'] = reddit_df['link_id'].astype(str)\n",
    "reddit_df['parent_id'] = reddit_df['parent_id'].astype(str)\n",
    "reddit_df['id'] = reddit_df['id'].astype(str)\n",
    "reddit_df['subreddit_id'] = reddit_df['subreddit_id'].astype(str)\n",
    "reddit_df['moderation\\r'] = reddit_df['moderation\\r'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove \\r from column names\n",
    "reddit_df.columns = reddit_df.columns.str.strip()\n",
    "\n",
    "# Strip \\r and other whitespace characters from a specific column (e.g., 'column_name')\n",
    "reddit_df['Topic'] = reddit_df['Topic'].str.strip()\n",
    "\n",
    "reddit_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract date from datetime stamp\n",
    "reddit_df['timestamp'] = reddit_df['timestamp'].dt.date\n",
    "reddit_df['timestamp'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Scoring - LionGuard\n",
    "\n",
    "We split this into 1 small test batch, and 1 larger one for running the full dataset.\n",
    "\n",
    "### Small Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_reddit_df = reddit_df[0:5]\n",
    "\n",
    "small_reddit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download model config\n",
    "repo_path = \"govtech/lionguard-v1\"\n",
    "config_path = hf_hub_download(repo_id=repo_path, filename=\"config.json\")\n",
    "with open(config_path, 'r') as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(device, data):\n",
    "    # Load the model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config['embedding']['tokenizer'])\n",
    "    model = AutoModel.from_pretrained(config['embedding']['model'])\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # Generate the embeddings\n",
    "    batch_size = config['embedding']['batch_size']\n",
    "    num_batches = int(np.ceil(len(data)/batch_size))\n",
    "    output = []\n",
    "    for i in range(num_batches):\n",
    "        sentences = data[i*batch_size:(i+1)*batch_size]\n",
    "        encoded_input = tokenizer(sentences, max_length=config['embedding']['max_length'], padding=True, truncation=True, return_tensors='pt')\n",
    "        encoded_input.to(device)\n",
    "        with torch.no_grad():\n",
    "            model_output = model(**encoded_input)\n",
    "            sentence_embeddings = model_output[0][:, 0]\n",
    "        sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)\n",
    "        output.extend(sentence_embeddings.cpu().numpy())\n",
    "    \n",
    "    return np.array(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict2(batch_text):\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
    "    embeddings = get_embeddings(device, batch_text)\n",
    "    embeddings_df = pd.DataFrame(embeddings)\n",
    "\n",
    "    # Prepare input data\n",
    "    X_input = np.array(embeddings_df, dtype=np.float32)\n",
    "\n",
    "    # Define the classifiers we want to focus on\n",
    "    selected_categories = ['hateful', 'toxic']  # Only focus on 'hateful' and 'toxic'\n",
    "\n",
    "    # Load the classifiers\n",
    "    results = {}\n",
    "    for category in selected_categories:  # Only loop over selected_categories\n",
    "        # Download the classifier from HuggingFace hub\n",
    "        local_model_fp = hf_hub_download(repo_id=repo_path, filename=config['classifier'][category]['model_fp'])\n",
    "\n",
    "        # Run the inference\n",
    "        session = rt.InferenceSession(local_model_fp)\n",
    "        input_name = session.get_inputs()[0].name\n",
    "        outputs = session.run(None, {input_name: X_input})\n",
    "\n",
    "        # If calibrated, return only the prediction for the unsafe class\n",
    "        if config['classifier'][category]['calibrated']:\n",
    "            scores = [output[1] for output in outputs[1]]\n",
    "        else:\n",
    "            scores = outputs[1].flatten()\n",
    "\n",
    "        # Generate the predictions depending on the recommended threshold score\n",
    "        results[f'{category} Score'] = {  # Directly access 'hateful' and 'toxic' scores\n",
    "            'scores': scores,\n",
    "            'predictions': {\n",
    "                'high_recall': [1 if score >= config['classifier'][category]['threshold']['high_recall'] else 0 for score in scores]\n",
    "            }\n",
    "        }\n",
    "        results[f'{category} HR'] = results[f'{category} Score']['predictions']['high_recall']  # CHANGE 4: Only high_recall predictions\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(batch_text):\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
    "    embeddings = get_embeddings(device, batch_text)\n",
    "    embeddings_df = pd.DataFrame(embeddings)\n",
    "\n",
    "    # Prepare input data\n",
    "    X_input = np.array(embeddings_df, dtype=np.float32)\n",
    "\n",
    "    # Load the classifiers\n",
    "    results = {}\n",
    "    for category, details in config['classifier'].items():\n",
    "        # Download the classifier from HuggingFace hub\n",
    "        local_model_fp = hf_hub_download(repo_id=repo_path, filename=config['classifier'][category]['model_fp'])\n",
    "\n",
    "        # Run the inference\n",
    "        session = rt.InferenceSession(local_model_fp)\n",
    "        input_name = session.get_inputs()[0].name\n",
    "        outputs = session.run(None, {input_name: X_input})\n",
    "\n",
    "        # If calibrated, return only the prediction for the unsafe class\n",
    "        if config['classifier'][category]['calibrated']: \n",
    "            scores = [output[1] for output in outputs[1]]\n",
    "        else:\n",
    "            scores = outputs[1].flatten()\n",
    "        \n",
    "        # Generate the predictions depending on the recommended threshold score\n",
    "        results[category] = {\n",
    "            'scores': scores,\n",
    "            'predictions': {\n",
    "                'high_recall': [1 if score >= config['classifier'][category]['threshold']['high_recall'] else 0 for score in scores],\n",
    "                'balanced': [1 if score >= config['classifier'][category]['threshold']['balanced'] else 0 for score in scores],\n",
    "                'high_precision': [1 if score >= config['classifier'][category]['threshold']['high_precision'] else 0 for score in scores]\n",
    "            }\n",
    "        }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the text data and id from the DataFrame\n",
    "batch_id = small_reddit_df['id'].tolist()\n",
    "batch_text = small_reddit_df['text'].tolist()\n",
    "\n",
    "# Generate the scores and predictions\n",
    "results = predict(batch_text)\n",
    "\n",
    "# Prepare results for DataFrame\n",
    "output_data = []\n",
    "for i in range(len(batch_text)):\n",
    "    output_row = {\n",
    "        'id': batch_id[i],\n",
    "        'Text': batch_text[i],\n",
    "    }\n",
    "    # IMPT! THIS LOOP WILL PRODUCE 32 COLUMNS! COMMENT OUT IF NOT NEEDED!\n",
    "    for category in results.keys():\n",
    "        # scores\n",
    "        output_row[f'{category} Score'] = results[category]['scores'][i]\n",
    "        # predictions with highest recall\n",
    "        output_row[f'{category} HR'] = results[category]['predictions']['high_recall'][i]\n",
    "        # balanced predictions\n",
    "        output_row[f'{category} B'] = results[category]['predictions']['balanced'][i]\n",
    "        # predictions with highest precision\n",
    "        output_row[f'{category} HP'] = results[category]['predictions']['high_precision'][i]\n",
    "    output_data.append(output_row)\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "small_results_df = pd.DataFrame(output_data)\n",
    "# Set display option to show all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# get results table\n",
    "print(small_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See column names\n",
    "print(small_results_df.columns)\n",
    "\n",
    "# get id, hateful and toxic scores only\n",
    "condensed_small_results_df = small_results_df[['id', 'hateful Score', 'toxic Score']]\n",
    "\n",
    "condensed_small_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the 2 dataframes on 'id'\n",
    "small_hateful_and_toxic_results_df = pd.merge(small_reddit_df, condensed_small_results_df, on='id', how='inner')\n",
    "\n",
    "print(small_hateful_and_toxic_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand resolution to see full text\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "small_hateful_and_toxic_results_df[['text', 'hateful Score', 'toxic Score']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Lion Guard for a full dataset:\n",
    "\n",
    "Note: It can take up to 6 - 10 hours to run this on google colab's free GPU. Best to run on local GPU device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure text is clean and all entries are strings\n",
    "reddit_df['text'] = reddit_df['text'].fillna('').astype(str)\n",
    "batch_text = reddit_df['text'].tolist()\n",
    "batch_id = reddit_df['id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the scores and predictions\n",
    "results = predict2(batch_text)\n",
    "\n",
    "# Prepare results for DataFrame\n",
    "output_data = []\n",
    "for i in range(len(batch_text)):\n",
    "    output_row = {\n",
    "        'id': batch_id[i],\n",
    "        'Text': batch_text[i],\n",
    "    }\n",
    "\n",
    "    # Directly add 'hateful Score', 'hateful HR', 'toxic Score', and 'toxic HR' to the output\n",
    "    output_row['hateful Score'] = results['hateful Score']['scores'][i]  \n",
    "    output_row['hateful HR'] = results['hateful HR'][i]  \n",
    "    output_row['toxic Score'] = results['toxic Score']['scores'][i]  \n",
    "    output_row['toxic HR'] = results['toxic HR'][i] \n",
    "\n",
    "    output_data.append(output_row)\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "results_df = pd.DataFrame(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See column names\n",
    "print(results_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get id, hateful and toxic scores only\n",
    "condensed_results_df = results_df[['id', 'hateful Score', 'hateful HR',  'toxic Score', 'toxic HR']]\n",
    "\n",
    "condensed_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the 2 dataframes on 'id'\n",
    "hateful_and_toxic_results_df = pd.merge(reddit_df, condensed_results_df, on='id', how='inner')\n",
    "\n",
    "hateful_and_toxic_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of rows\n",
    "hateful_and_toxic_results_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = reddit_df\n",
    "df = clean_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Define a function to remove stopwords from a single text\n",
    "def remove_stopwords(text):\n",
    "    # Tokenize the text\n",
    "    words = word_tokenize(text)\n",
    "    # Filter out stopwords\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    # Join the remaining words back into a single string\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Apply the function to the 'comments' column\n",
    "df['cleaned_comments'] = df['text'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_df=0.8, min_df=2, stop_words='english')\n",
    "data_vectorized = vectorizer.fit_transform(df['cleaned_comments'])\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=10, random_state=42)\n",
    "lda.fit(data_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display the top words for each topic\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {topic_idx}:\")\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "# Number of words to display per topic\n",
    "no_top_words = 10\n",
    "\n",
    "# Get the feature names (words)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Display the topics\n",
    "display_topics(lda, feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seeded LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    reddit_df = pd.read_csv(path, lineterminator='\\n', encoding='utf8')\n",
    "    return reddit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_gensim(text):\n",
    "    \"\"\"Tokenizes and processes the text using Gensim.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        return ' '.join(gu.simple_preprocess(text))\n",
    "    else:\n",
    "        return ''  # Return an empty string for non-string inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df):\n",
    "    df['clean_text'] = df['text'].str.lower()\n",
    "    print(\"cleaned_lower\")\n",
    "    df['clean_text'] = df['clean_text'].str.replace(r'[^a-zA-Z\\s]', ' ',regex=True) \n",
    "    df['clean_text'] = df['clean_text'].str.replace(r'\\s{2,}', ' ',regex=True)   \n",
    "    print(\"cleaned_regex\") \n",
    "    df['clean_text'] = df['clean_text'].apply(preprocess_gensim)\n",
    "    print(\"cleaned_preprocessed\")\n",
    "    df['clean_text'] = df['clean_text'].apply(word_tokenize)\n",
    "    print(\"cleaned_tokenized\")\n",
    "    df['clean_text'] = df['clean_text'].apply(lambda x:[word for word in x if word not in stopwords.words(\"english\") and word.isalpha()])\n",
    "    print(\"cleaned_stopwords\")\n",
    "    df['clean_text'] = df['clean_text'].apply(lambda x: [WordNetLemmatizer().lemmatize(word) for word in x])\n",
    "    print(\"cleaned_Lemmatized\")\n",
    "    df['clean_text'] = df['clean_text'].apply(lambda x: [word for word in x if nltk.pos_tag([word])[0][1] == 'NN'])\n",
    "    print(\"cleaned_tagged\")\n",
    "    df = df[df['clean_text'].map(lambda x: len(x)) > 1].reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionary(reddit_df):\n",
    "    texts = reddit_df['clean_text']\n",
    "    id2word = corpora.Dictionary(texts)\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "    return texts, id2word, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mallet(system,folder_path):\n",
    "    os.environ['MALLET_HOME']=folder_path\n",
    "    if system == 'windows': mallet_path = folder_path+\"\\\\bin\\\\mallet.bat\"\n",
    "    elif system == 'mac': mallet_path = folder_path+\"/bin/mallet\"\n",
    "    return mallet_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function for topic modeling\n",
    "def topic_modelling(model, corpus, texts, data, seed_topics):\n",
    "    output_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(model[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        print(f\"Document {i}, Topics: {row}\")\n",
    "\n",
    "        # If the text is empty, classify it as 'Others'\n",
    "        if not texts[i]:\n",
    "            output_df = pd.concat([\n",
    "                output_df,\n",
    "                pd.DataFrame([[10, 'Others', 1.000, '']], columns=['Topic Number', 'Topic', 'Perc_Contribution', 'Topic_Keywords'])\n",
    "            ], ignore_index=True)\n",
    "        else:\n",
    "            for j, (topic_num, prop_topic) in enumerate(row):\n",
    "                if j == 0:  # Dominant topic (highest contribution)\n",
    "                    wp = model.show_topic(topic_num)\n",
    "                    topic_keywords = \", \".join([word for word, prop in wp])\n",
    "\n",
    "                    # Map the topic number to a predefined topic using seed_topics\n",
    "                    topic_name = seed_topics.get(int(topic_num), 'Unknown')\n",
    "\n",
    "                    output_df = pd.concat([\n",
    "                        output_df,\n",
    "                        pd.DataFrame([[int(topic_num), topic_name, round(prop_topic, 4), topic_keywords]], \n",
    "                                      columns=['Topic Number', 'Topic', 'Perc_Contribution', 'Topic_Keywords'])\n",
    "                    ], ignore_index=True)\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "    # Concatenate the original data with the topic modeling results\n",
    "    output_df = pd.concat([data, output_df], axis=1)\n",
    "\n",
    "    # Remove any unnecessary columns such as 'clean_text' if needed\n",
    "    output_df = output_df.drop(['clean_text', 'Perc_Contribution', 'Topic_Keywords'], axis=1, errors='ignore')\n",
    "\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your seed topics\n",
    "seed_topics = {\n",
    "    0: \"Political\",\n",
    "    1: \"Covid-19\",\n",
    "    2: \"Race & Religion\",\n",
    "    3: \"Transport\",\n",
    "    4: \"Relationships\",\n",
    "    5: \"Crime\",\n",
    "    6: \"Housing\",\n",
    "    7: \"Education\",\n",
    "    8: \"Work\"\n",
    "}\n",
    "\n",
    "# Define the seed words for each topic\n",
    "seed_words = {\n",
    "    \"Political\": [\"ge\", \"general election\", \"affair\", \"mp\", \"politician\", \"politics\"],\n",
    "    \"Covid-19\": [\"covid-19\", \"infection\", \"vaccine\", \"lockdown\", \"circuit breaker\", \"mask\", \"cough\"],\n",
    "    \"Race & Religion\": [\"chinese\", \"malay\", \"indian\", \"angmoh\", \"culture\", \"christian\", \"buddhist\", \"muslim\", \"racist\", \"CECA\"],\n",
    "    \"Transport\": [\"breakdown\", \"train\", \"mrt\", \"lrt\", \"bus\", \"simplygo\"],\n",
    "    \"Relationships\": [\"relationships\", \"husband\", \"wife\", \"bf\", \"gf\", \"breakup\", \"cheat\", \"affair\", \"lover\", \"divorce\", \"love\"],\n",
    "    \"Crime\": [\"crime\", \"case\", \"police\", \"murder\", \"kill\", \"death\", \"scam\"],\n",
    "    \"Housing\": [\"hdb\", \"price\", \"bto\", \"resale\"],\n",
    "    \"Education\": [\"student\", \"psle\", \"study\", \"alevel\", \"olevel\", \"exam\", \"school\"],\n",
    "    \"Work\": [\"ot\", \"salary\", \"unemployed\", \"boss\", \"job\", \"laoban\", \"colleague\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df = hateful_and_toxic_results_df\n",
    "reddit_df_processed = preprocessing(reddit_df) # Can take up to a few hours to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, id2word, corpus = create_dictionary(reddit_df_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Download MALLET from here: https://mallet.cs.umass.edu/download.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mallet_path = r\" \" # Insert Path to Mallet here\n",
    "\n",
    "os.environ['MALLET_HOME'] = \" \" # Eg. r'C:\\Users\\mallet-2.0.8\\mallet-2.0.8'\n",
    "os.environ['PATH'] = os.environ['PATH'] + os.pathsep + \" \" # Eg r'C:\\Users\\mallet-2.0.8\\mallet-2.0.8\\bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mallet(mallet_path, num_topics, id2word, corpus):\n",
    "    # Use Gensim's wrapper for MALLET\n",
    "    return ldamallet.LdaMallet(\n",
    "        mallet_path=mallet_path, \n",
    "        corpus=corpus, \n",
    "        num_topics=num_topics, \n",
    "        id2word=id2word\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mallet = create_mallet(mallet_path=mallet_path, num_topics=10, id2word=id2word, corpus=corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = topic_modelling(model=mallet,corpus=corpus,texts=texts,data=reddit_df, seed_topics = seed_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.to_csv('topic_model_results.csv', index=False)\n",
    "output_df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coherence Scores:\n",
    "\n",
    "## c_v method\n",
    "# Create the CoherenceModel for the Mallet model\n",
    "coherence_model_mallet = CoherenceModel(model=mallet, texts=texts, dictionary=id2word, coherence='c_v')\n",
    "# Compute the coherence score\n",
    "coherence_score = coherence_model_mallet.get_coherence()\n",
    "print(f'Coherence Score: {coherence_score}')\n",
    "\n",
    "\n",
    "## u_mass method\n",
    "# Assuming texts is a list of tokenized texts\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "seeded_topics = list(seed_words.values())\n",
    "# Create the CoherenceModel with corpus and dictionary for 'u_mass'\n",
    "cm = CoherenceModel(topics=seeded_topics, corpus=corpus, dictionary=id2word, coherence='u_mass')\n",
    "coherence = cm.get_coherence()  # get coherence value\n",
    "coherence\n",
    "\n",
    "## c_uci method\n",
    "cm = CoherenceModel(topics=seeded_topics, texts=texts, dictionary=id2word, coherence='c_uci')\n",
    "cm.get_coherence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations:\n",
    "\n",
    "### Daily Average Hatefulness and Toxicity Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv('../../data/topic_model_results.csv',  lineterminator='\\n', encoding='utf8')\n",
    "reddit_df = pd.read_csv('../../data/topic_model_results.csv',  lineterminator='\\n', encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove \\r from column names\n",
    "results_df.columns = results_df.columns.str.strip()\n",
    "\n",
    "# Strip \\r and other whitespace characters from a specific column (e.g., 'column_name')\n",
    "results_df['Topic'] = results_df['Topic'].str.strip()\n",
    "\n",
    "results_df.head()\n",
    "\n",
    "# rename new columns\n",
    "results_df.rename(columns={\n",
    "    'hateful Score': 'hateful_score',\n",
    "    'hateful HR': 'hateful_prediction',\n",
    "    'toxic Score': 'toxic_score',\n",
    "    'toxic HR': 'toxic_prediction',\n",
    "    'Topic Number': 'topic_number',\n",
    "    'Topic': 'topic'\n",
    "}, inplace=True)\n",
    "\n",
    "results_df.head()\n",
    "\n",
    "results_df['moderation'] = results_df['moderation'].apply(ast.literal_eval)\n",
    "\n",
    "# Create boolean flags for collapsed, deleted, low-score, removed, and controversial comments\n",
    "results_df['is_collapsed'] = results_df['moderation'].apply(lambda x: x.get('collapsed') == True)\n",
    "results_df['is_deleted'] = results_df['moderation'].apply(lambda x: x.get('collapsed_reason_code') == 'DELETED')\n",
    "results_df['is_controversial'] = results_df['moderation'].apply(lambda x: x.get('controversiality') == 1)\n",
    "\n",
    "results_df['is_controversial'].head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data to appropriate datatypes\n",
    "results_df['text'] = results_df['text'].astype(str)\n",
    "results_df['timestamp'] = pd.to_datetime(results_df['timestamp'], format='%d/%m/%Y %H:%M')\n",
    "results_df['username'] = results_df['username'].astype(str)\n",
    "results_df['link'] = results_df['link'].astype(str)\n",
    "results_df['link_id'] = results_df['link_id'].astype(str)\n",
    "results_df['parent_id'] = results_df['parent_id'].astype(str)\n",
    "results_df['id'] = results_df['id'].astype(str)\n",
    "results_df['subreddit_id'] = results_df['subreddit_id'].astype(str)\n",
    "results_df['moderation'] = results_df['moderation'].tolist()\n",
    "results_df['hateful_score'] = results_df['hateful_score'].astype('float32')\n",
    "results_df['hateful_prediction'] = results_df['hateful_prediction'].astype('int16')\n",
    "results_df['toxic_score'] = results_df['toxic_score'].astype('float32')\n",
    "results_df['toxic_prediction'] = results_df['toxic_prediction'].astype('int16')\n",
    "results_df['topic_number'] = results_df['topic_number'].astype('category')\n",
    "results_df['topic'] = results_df['topic'].astype('category')\n",
    "\n",
    "\n",
    "# Group by year and month to track moderation actions over time\n",
    "results_df['year_month'] = results_df['timestamp'].dt.to_period('M')\n",
    "\n",
    "# extract date from datetime stamp\n",
    "results_df['timestamp'] = results_df['timestamp'].dt.date\n",
    "\n",
    "results_df['timestamp'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by day and calculate the average score\n",
    "average_hatefulness_and_toxicity_per_day_df = results_df.groupby(results_df['timestamp']).agg(\n",
    "    average_hateful_score=('hateful_score', 'mean'), \n",
    "    average_toxic_score=('toxic_score', 'mean')\n",
    ").reset_index()\n",
    "average_hatefulness_and_toxicity_per_day_df.head()\n",
    "\n",
    "average_hatefulness_and_toxicity_per_day_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'timestamp' is a datetime object\n",
    "average_hatefulness_and_toxicity_per_day_df['timestamp'] = pd.to_datetime(average_hatefulness_and_toxicity_per_day_df['timestamp'])\n",
    "\n",
    "average_hatefulness_and_toxicity_per_day_df[average_hatefulness_and_toxicity_per_day_df['timestamp'].dt.date == pd.to_datetime('2023-04-30').date()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first DataFrame\n",
    "plt.plot(average_hatefulness_and_toxicity_per_day_df['timestamp'], average_hatefulness_and_toxicity_per_day_df['average_hateful_score'], label='Hateful Score', color='red', alpha=0.7)\n",
    "\n",
    "# Plot the second DataFrame\n",
    "plt.plot(average_hatefulness_and_toxicity_per_day_df['timestamp'], average_hatefulness_and_toxicity_per_day_df['average_toxic_score'], label='Toxic Score', color='green', alpha=0.7)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Average Scores')\n",
    "plt.title('Daily Average Hatefulness and Toxic Scores')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add a legend to differentiate the lines\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Comments per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of comments per day\n",
    "num_of_comments_per_day_df = results_df.groupby('timestamp')['id'].count()\n",
    "num_of_comments_per_day_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot number of comments against time\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(num_of_comments_per_day_df.index, num_of_comments_per_day_df.values)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Comments')\n",
    "plt.title('Number of Comments per Day')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toxicity Score Distribution by subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits_of_interest = results_df['subreddit_name'].unique()\n",
    "subreddits_of_interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the aesthetic style of the plots\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create a box plot with a custom color palette\n",
    "plt.figure(figsize=(14, 8))\n",
    "palette = {'r/singapore': \"#4c72b0\", 'r/singaporeraw': \"#55a868\", 'r/singaporehappenings': \"#c44e52\"}\n",
    "ax = sns.boxplot(data=results_df, x='subreddit_name', y='toxic_score', order=subreddits_of_interest, palette=palette, whis = 200)\n",
    "\n",
    "# Annotate median, first quartile (Q1), and third quartile (Q3) for each subreddit\n",
    "for i, subreddit in enumerate(subreddits_of_interest):\n",
    "    subreddit_data = results_df[results_df['subreddit_name'] == subreddit]['toxic_score']\n",
    "    median = subreddit_data.median()\n",
    "    q1 = subreddit_data.quantile(0.25)\n",
    "    q3 = subreddit_data.quantile(0.75)\n",
    "    min_val = subreddit_data.min()\n",
    "    max_val = subreddit_data.max()\n",
    "    \n",
    "    # Median label\n",
    "    ax.text(i, median, f'Median\\n{median:.2f}', ha='center', va='center', color='white', fontweight='bold',\n",
    "            bbox=dict(facecolor=palette[subreddit], edgecolor='none', boxstyle='round,pad=0.3'))\n",
    "    # Q1 and Q3 labels\n",
    "    ax.text(i, q1, f'25th percentile\\n{q1:.2f}', ha='right' , va= 'top', color=palette[subreddit], fontsize=13)\n",
    "    ax.text(i, q3, f'75th percentile\\n{q3:.2f}', ha='right', va='bottom', color=palette[subreddit], fontsize=13)\n",
    "    # Min and Max labels\n",
    "    ax.text(i, min_val, f'Min\\n{min_val:.2f}', ha='center', va='top', color='black', fontsize=12)\n",
    "    ax.text(i, max_val, f'Max\\n{max_val:.2f}', ha='center', va='bottom', color='black', fontsize=12)\n",
    "\n",
    "# Set title and labels\n",
    "plt.title('Toxic Score Distribution by Subreddit', fontsize=18, fontweight='bold')\n",
    "plt.xlabel('Subreddit', fontsize=14)\n",
    "plt.ylabel('Toxic Score', fontsize=14)\n",
    "\n",
    "# Improve spacing and add legend for clarity\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Toxic Score per Month by Subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure year_month is in datetime format for consistent plotting\n",
    "results_df['year_month'] = pd.to_datetime(results_df['year_month'].astype(str))\n",
    "\n",
    "# Aggregate to calculate average toxic_score per subreddit per month\n",
    "monthly_avg_toxic = results_df.groupby(['year_month', 'subreddit_name'])['toxic_score'].mean().reset_index()\n",
    "monthly_avg_toxic.rename(columns={'toxic_score': 'average_toxic_score'}, inplace=True)\n",
    "\n",
    "# Define a custom color palette for the subreddits\n",
    "custom_palette = {\n",
    "    'r/singapore': '#4c72b0',\n",
    "    'r/singaporeraw': '#55a868',\n",
    "    'r/singaporehappenings': '#c44e52'\n",
    "}\n",
    "\n",
    "# Plot using seaborn with the custom color palette\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.lineplot(data=monthly_avg_toxic, x='year_month', y='average_toxic_score', hue='subreddit_name', \n",
    "             marker='o', palette=custom_palette)\n",
    "\n",
    "# Formatting the plot\n",
    "plt.xlabel('Year-Month')\n",
    "plt.ylabel('Average Toxic Score')\n",
    "plt.title('Average Toxic Score per Month by Subreddit')\n",
    "plt.xticks(rotation=45)  # Rotate for readability\n",
    "plt.legend(title='Subreddit', bbox_to_anchor=(1.05, 1), loc='upper left')  # Place legend outside the plot\n",
    "\n",
    "# Add gridlines\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monthly Comment Count by Subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate to count the number of entries per subreddit per month\n",
    "monthly_comment_count = results_df.groupby(['year_month', 'subreddit_name']).size().reset_index(name='comment_count')\n",
    "\n",
    "# Plot using seaborn to handle grouping by subreddit\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.lineplot(data=monthly_comment_count, x='year_month', y='comment_count', hue='subreddit_name', marker='o', palette=custom_palette)\n",
    "\n",
    "# Formatting the plot\n",
    "plt.xlabel('Year-Month')\n",
    "plt.ylabel('Number of Comments')\n",
    "plt.title('Monthly Comment Count by Subreddit')\n",
    "plt.xticks(rotation=45)  # Rotate for readability\n",
    "plt.legend(title='Subreddit', bbox_to_anchor=(1.05, 1), loc='upper left')  # Place legend outside the plot\n",
    "# Add gridlines\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Comments grouped by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of comments by year and month \n",
    "num_of_comments_per_month_by_year_df = reddit_df.groupby(['year', 'month'])['id'].count().reset_index()\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Loop through each year and plot number of comments per month\n",
    "for year in num_of_comments_per_month_by_year_df['year'].unique():\n",
    "    data_by_year = num_of_comments_per_month_by_year_df[num_of_comments_per_month_by_year_df['year'] == year]\n",
    "    plt.plot(data_by_year['month'], data_by_year['id'], marker='o', label=str(year))\n",
    "\n",
    "# Customizing the plot\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Number of Comments')\n",
    "plt.title('Number of Comments per Month Grouped by Year')\n",
    "plt.xticks(ticks=range(1, 13), labels=['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
    "plt.legend(title='Year')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hateful and Toxic Score by timestamp boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dates to filter for\n",
    "dates_to_filter = [\n",
    "    datetime.date(2021, 12, 7),\n",
    "    datetime.date(2020, 7, 10),\n",
    "    datetime.date(2023, 7, 17),\n",
    "    datetime.date(2021, 10, 20)\n",
    "]\n",
    "\n",
    "# Filter DataFrame for these specific dates\n",
    "big_events_vs_average_day_df = results_df[results_df['timestamp'].isin(dates_to_filter)]\n",
    "\n",
    "big_events_vs_average_day_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to datetime if it's not already\n",
    "big_events_vs_average_day_df['timestamp'] = pd.to_datetime(big_events_vs_average_day_df['timestamp'], errors='coerce')\n",
    "\n",
    "# Now, convert it to string format\n",
    "big_events_vs_average_day_df['timestamp'] = big_events_vs_average_day_df['timestamp'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Define a mapping for renaming timestamps\n",
    "timestamp_mapping = {\n",
    "    '2024-01-01': 'Event A',\n",
    "    '2024-01-02': 'Event B',\n",
    "    '2024-01-03': 'Event C',\n",
    "    '2024-01-04': 'Event D'\n",
    "}\n",
    "\n",
    "# Rename the timestamps in the DataFrame using the mapping\n",
    "big_events_vs_average_day_df['timestamp'] = big_events_vs_average_day_df['timestamp'].replace(timestamp_mapping)\n",
    "\n",
    "# Ensure that the timestamp column is treated as a categorical variable\n",
    "big_events_vs_average_day_df['timestamp'] = big_events_vs_average_day_df['timestamp'].astype('category')\n",
    "\n",
    "# Melt the DataFrame to have a long format suitable for seaborn\n",
    "melted_df = big_events_vs_average_day_df.melt(\n",
    "    id_vars=['timestamp'],\n",
    "    value_vars=['hateful_score', 'toxic_score'],\n",
    "    var_name='score_type',\n",
    "    value_name='score_value'\n",
    ")\n",
    "\n",
    "# Create a color palette\n",
    "palette = {\n",
    "    'hateful_score': 'red',\n",
    "    'toxic_score': 'green'\n",
    "}\n",
    "\n",
    "# Create a boxplot with custom colors\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=melted_df, x='timestamp', y='score_value', hue='score_type', palette=palette)\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Boxplot of Hateful Scores and Toxic Scores by Timestamp')\n",
    "plt.xlabel('Timestamp')\n",
    "plt.ylabel('Score Value')\n",
    "plt.legend(title='Score Type')\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of Toxicity and Hatefulness Scores by Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_stats = results_df.groupby('topic')[['toxic_score', 'hateful_score']].agg(['mean', 'median', 'std']).reset_index()\n",
    "topic_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the figure and axes for two subplots (side by side)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# First subplot: Boxplot for 'toxic_score'\n",
    "sns.boxplot(data=results_df, x='topic', y='toxic_score', ax=axes[0])\n",
    "axes[0].set_title('Distribution of Toxicity Scores by Topic')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Second subplot: Boxplot for 'hateful_score'\n",
    "sns.boxplot(data=results_df, x='topic', y='hateful_score', ax=axes[1])\n",
    "axes[1].set_title('Distribution of Hatefulness Scores by Topic')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(wspace=0.2)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of Toxic Score by content category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordcloud based on content category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = reddit_df\n",
    "# get the title of the post that the comment is in (to sense what the post is talking about)\n",
    "def get_title(link):\n",
    "    parts = link.split('/')\n",
    "    return parts[5]\n",
    "\n",
    "# get the post that the comment is in (some posts have the same title)\n",
    "def get_post(link):\n",
    "    parts = link.split('/')\n",
    "    subreddit = parts[2]\n",
    "    post_id = parts[4]\n",
    "    title = parts[5]\n",
    "    return subreddit + ',' + post_id + ',' + title\n",
    "\n",
    "df['title'] = df['link'].apply(get_title)\n",
    "df['post'] = df['link'].apply(get_post)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = df['post'].value_counts()\n",
    "\n",
    "# getting the top 10 posts with the most number of comments\n",
    "print(counts[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Post: singapore general elections 2020 polling results \"\"\"\n",
    "\n",
    "\n",
    "ge2020_poll_results = df[df['post'] == 'singapore,homxdq,singapore_general_elections_2020_polling_results']\n",
    "print(ge2020_poll_results['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "From the posts, the two large topics that come out are:\n",
    "\n",
    "Singapore general elections 2020\n",
    "posts (based on top 20):\n",
    "'singapore_general_elections_2020_polling_results'\n",
    "'ge2020_nomination_day_mega_thread'\n",
    "'ge_2020_political_debate_megathread'\n",
    "total number of comments (based on top 20 posts): 1154\n",
    "COVID-19, especially on dining restrictions\n",
    "posts (based on top 20):\n",
    "'no_dining_in_social_gatherings_capped_at_2_people...'\n",
    "'no_dining_in_social_group_sizes_cut_to_2_from...'\n",
    "'pm_lee_to_address_nation_on_covid19_situation_and...'\n",
    "'stabilisation_phase_extended_to_nov_21_more_time...'\n",
    "'covid19_diningin_group_size_limit_at_regular_fb...'\n",
    "'pm_lee_announces_new_stricter_restrictions_to...'\n",
    "'those_unvaccinated_against_covid19_will_no_longer...'\n",
    "'live_pm_lee_addresses_nation_on_covid19_situation...'\n",
    "'covid19_task_force_evaluating_timing_and_scope_of...'\n",
    "total number of comments (based on top 20 posts): 934"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = reddit_df\n",
    "df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg = df_2[df_2['link'].str.startswith('/r/singapore')].reset_index(drop=True)\n",
    "print(sg['link'][0])\n",
    "sgraw = df_2[df_2['link'].str.startswith('/r/SingaporeRaw')].reset_index(drop=True)\n",
    "print(sgraw['link'][0])\n",
    "sghap = df_2[df_2['link'].str.startswith('/r/singaporehappenings')].reset_index(drop=True)\n",
    "print(sghap['link'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the title of the post that the comment is in (to sense what the post is talking about)\n",
    "def get_title(link):\n",
    "    parts = link.split('/')\n",
    "    return parts[5]\n",
    "\n",
    "# get the post that the comment is in (some posts have the same title)\n",
    "def get_post(link):\n",
    "    parts = link.split('/')\n",
    "    subreddit = parts[2]\n",
    "    post_id = parts[4]\n",
    "    title = parts[5]\n",
    "    return subreddit + ',' + post_id + ',' + title\n",
    "\n",
    "df_2['title'] = df_2['link'].apply(get_title)\n",
    "df_2['post'] = df_2['link'].apply(get_post)\n",
    "df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = df_2['post'].value_counts()\n",
    "\n",
    "# getting the top 10 posts with the most number of comments\n",
    "print(counts[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ge2020_posts = df_2[df_2['post'].isin([\n",
    "    'singapore,homxdq,singapore_general_elections_2020_polling_results',\n",
    "    'singapore,hie65n,ge2020_nomination_day_mega_thread',\n",
    "    'singapore,hj8h5p,ge_2020_political_debate_megathread',\n",
    "])]\n",
    "\n",
    "covid_posts = df_2[df_2['post'].isin([\n",
    "    'singapore,nc0vwe,no_dining_in_social_gatherings_capped_at_2_people',  \n",
    "    'singapore,onx8xr,no_dining_in_social_group_sizes_cut_to_2_from',\n",
    "    'singapore,qbyerz,stabilisation_phase_extended_to_nov_21_more_time',  \n",
    "    'singapore,puh01k,covid19_diningin_group_size_limit_at_regular_fb',\n",
    "    'singapore,fu4ch0,pm_lee_announces_new_stricter_restrictions_to', \n",
    "    'singapore,q4e96u,those_unvaccinated_against_covid19_will_no_longer',   \n",
    "    'singapore,tlvwx9,live_pm_lee_addresses_nation_on_covid19_situation',   \n",
    "    'singapore,o104h0,covid19_task_force_evaluating_timing_and_scope_of',\n",
    "    'singapore,nosjic,megathread_pm_lee_delivers_national_address_on'   \n",
    "])]\n",
    "\n",
    "nationalday_posts = df_2[df_2['post'].isin([\n",
    "    'singapore,wjxxdd,megathread_national_day_parade_2022',\n",
    "    'singapore,wtc8jy,megathread_national_day_rally_2022',\n",
    "           \n",
    "])]\n",
    "\n",
    "lifestyle_stress_posts = df_2[df_2['post'].isin([\n",
    "    'singapore,hs2ynr,this_is_basically_the_entirety_of_an_average',\n",
    "    'singapore,q3204h,whats_the_point_of_bringing_a_life_into_singapore '\n",
    "           \n",
    "])]\n",
    "\n",
    "violent_posts = df_2[df_2['post'].isin([\n",
    "    'singapore,on8f8m,river_valley_high_school_student_killed_on_campus',\n",
    "    'singapore,nj3o0j,someone_is_attacked_on_mrt'   \n",
    "])]\n",
    "\n",
    "\n",
    "misc_posts = df_2[df_2['post'].isin([\n",
    "    'singapore,on2hbu,using_only_emojis_which_town_are_you_from'\n",
    "])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of DataFrames and their titles\n",
    "dfs = {\n",
    "    'ge2020_posts': ge2020_posts,\n",
    "    'covid_posts': covid_posts,\n",
    "    'nationalday_posts': nationalday_posts,\n",
    "    'lifestyle_stress_posts': lifestyle_stress_posts,\n",
    "    'violent_posts': violent_posts,\n",
    "    'misc_posts': misc_posts\n",
    "}\n",
    "\n",
    "# Initialize lists to store results\n",
    "categories = []\n",
    "scores = []\n",
    "means = []\n",
    "modes = []\n",
    "medians = []\n",
    "percentile_25 = []\n",
    "percentile_75 = []\n",
    "\n",
    "# Loop through each DataFrame and calculate statistics\n",
    "for category, df in dfs.items():\n",
    "    # Calculate mean, mode, median, 25th percentile, and 75th percentile values\n",
    "    for score_type in ['hateful Score', 'hateful HR', 'toxic Score', 'toxic HR']:\n",
    "        means.append(df[score_type].mean())\n",
    "        modes.append(df[score_type].mode()[0])\n",
    "        medians.append(df[score_type].median())\n",
    "        percentile_25.append(df[score_type].quantile(0.25))\n",
    "        percentile_75.append(df[score_type].quantile(0.75))\n",
    "        \n",
    "        # Add the category and score type to corresponding lists\n",
    "        categories.append(category)\n",
    "        scores.append(score_type)\n",
    "\n",
    "# Create the new DataFrame\n",
    "data = {\n",
    "    'Content Category': categories,\n",
    "    'Score Type': scores,\n",
    "    'Mean': means,\n",
    "    'Mode': modes,\n",
    "    'Median': medians,\n",
    "    '25th Percentile': percentile_25,\n",
    "    '75th Percentile': percentile_75\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "new_df = pd.DataFrame(data)\n",
    "\n",
    "# Display the new DataFrame\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the individual DataFrames into one for plotting\n",
    "dfs = {\n",
    "    'ge2020_posts': ge2020_posts,\n",
    "    'covid_posts': covid_posts,\n",
    "    'nationalday_posts': nationalday_posts,\n",
    "    'lifestyle_stress_posts': lifestyle_stress_posts,\n",
    "    'violent_posts': violent_posts,\n",
    "    'misc_posts': misc_posts\n",
    "}\n",
    "\n",
    "# Add a 'Content Category' column to each DataFrame and concatenate them\n",
    "for category, df in dfs.items():\n",
    "    df['Content Category'] = category\n",
    "\n",
    "# Concatenate all DataFrames\n",
    "combined_df = pd.concat(dfs.values(), ignore_index=True)\n",
    "\n",
    "# Melt the DataFrame to bring it into a long format for Seaborn\n",
    "long_df = combined_df.melt(id_vars=['Content Category'], \n",
    "                           value_vars=['hateful Score', 'toxic Score'],\n",
    "                           var_name='Score Type', \n",
    "                           value_name='Score')\n",
    "\n",
    "# Filter for hateful Score and toxic Score\n",
    "hateful_df = long_df[long_df['Score Type'] == 'hateful Score']\n",
    "toxic_df = long_df[long_df['Score Type'] == 'toxic Score']\n",
    "\n",
    "# Plot the box plot for hateful Score\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=hateful_df, x='Score Type', y='Score', hue='Content Category')\n",
    "plt.title('Distribution of Hateful Score by Content Category')\n",
    "plt.xlabel('Score Type')\n",
    "plt.ylabel('Score')\n",
    "plt.legend(title='Content Category', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot the box plot for toxic Score\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=toxic_df, x='Score Type', y='Score', hue='Content Category')\n",
    "plt.title('Distribution of Toxic Score by Content Category')\n",
    "plt.xlabel('Score Type')\n",
    "plt.ylabel('Score')\n",
    "plt.legend(title='Content Category', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordcloud of most frequent qord for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure stopwords are downloaded (if not already done)\n",
    "nltk.download('stopwords')\n",
    "common_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "# Example data structure for multiple posts, adjust accordingly\n",
    "posts_data = [\n",
    "    (ge2020_posts, 'GE2020 Posts'),\n",
    "    (covid_posts, 'COVID Posts'),\n",
    "    (nationalday_posts, 'National Day Posts'),\n",
    "    (lifestyle_stress_posts, 'Lifestyle Stress Posts'),\n",
    "    (violent_posts, 'Violent Posts'),\n",
    "    (misc_posts, 'Miscellaneous Posts')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of subplots based on the length of posts_data\n",
    "n_posts = len(posts_data)\n",
    "n_cols = 2\n",
    "n_rows = math.ceil(n_posts / n_cols)\n",
    "\n",
    "# Function to preprocess text by removing common stopwords\n",
    "def preprocess_text(text):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in common_stopwords]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Preprocess each DataFrame\n",
    "for i, (df, _) in enumerate(posts_data):\n",
    "    df['text'] = df['text'].fillna('').apply(preprocess_text)  # Remove NaN values and apply stop word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of subplots based on the length of posts_data\n",
    "n_posts = len(posts_data)\n",
    "n_cols = 2\n",
    "n_rows = math.ceil(n_posts / n_cols)\n",
    "\n",
    "plt.figure(figsize=(15, 7 * n_rows))\n",
    "\n",
    "# Loop through the DataFrames to generate word clouds\n",
    "for i, (df, title) in enumerate(posts_data):\n",
    "    # Create TF-IDF Vectorizer (without preprocessor)\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words='english', min_df=1, max_features=500)\n",
    "\n",
    "    # Fill NaN values and fit-transform the data\n",
    "    df['text'] = df['text'].fillna('')\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(df['text'].values)\n",
    "\n",
    "    # Sum the TF-IDF scores for each term across all documents\n",
    "    tfidf_sum = tfidf_matrix.sum(axis=0)\n",
    "\n",
    "    # Get words and corresponding TF-IDF scores\n",
    "    words = tfidf_vectorizer.get_feature_names_out()\n",
    "    tfidf_scores = tfidf_sum.A1  # Convert to a 1D array\n",
    "    word_scores = dict(zip(words, tfidf_scores))\n",
    "\n",
    "    # Generate the word cloud\n",
    "    wordcloud = WordCloud(width=1500, height=800, background_color='white', colormap='viridis').generate_from_frequencies(word_scores)\n",
    "\n",
    "    # Plot the word cloud\n",
    "    plt.subplot(n_rows, n_cols, i + 1)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')  # Hide the axes\n",
    "    plt.title(title)\n",
    "\n",
    "# Adjust layout and display\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure to hold the subplots\n",
    "plt.figure(figsize=(18, 12))\n",
    "\n",
    "# Loop through the DataFrames to generate word clouds\n",
    "for i, (df, title) in enumerate(posts_data):\n",
    "    # Filter for toxic comments with a score greater than 0\n",
    "    toxic_comments = df[df['toxic Score'] > 0]\n",
    "    \n",
    "    if toxic_comments.empty:  # Skip if there are no toxic comments\n",
    "        continue\n",
    "\n",
    "    # Create TF-IDF Vectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words='english', min_df=1, max_features=500)\n",
    "\n",
    "    # Fill NaN values and fit-transform the data\n",
    "    toxic_comments['text'] = toxic_comments['text'].fillna('')\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(toxic_comments['text'].values)\n",
    "\n",
    "    # Sum the TF-IDF scores for each term across all documents\n",
    "    tfidf_sum = tfidf_matrix.sum(axis=0)\n",
    "\n",
    "    # Get words and corresponding TF-IDF scores\n",
    "    words = tfidf_vectorizer.get_feature_names_out()\n",
    "    tfidf_scores = tfidf_sum.A1  # Convert to a 1D array\n",
    "    word_scores = dict(zip(words, tfidf_scores))\n",
    "\n",
    "    # Generate the word cloud\n",
    "    wordcloud = WordCloud(width=1500, height=800, background_color='white', colormap='viridis').generate_from_frequencies(word_scores)\n",
    "\n",
    "    # Plot the word cloud\n",
    "    plt.subplot(3, 2, i + 1)  # Arrange plots in 3 rows and 2 columns\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')  # Hide the axes\n",
    "    plt.title(title)\n",
    "\n",
    "# Adjust layout and display\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have separate DataFrames for each content category\n",
    "posts_data = [\n",
    "    (ge2020_posts, 'GE2020 Posts'),\n",
    "    (covid_posts, 'COVID Posts'),\n",
    "    (nationalday_posts, 'National Day Posts'),\n",
    "    (lifestyle_stress_posts, 'Lifestyle Stress Posts'),\n",
    "    (violent_posts, 'Violent Posts'),\n",
    "    (misc_posts, 'Miscellaneous Posts')\n",
    "]\n",
    "\n",
    "# Create a figure to hold the subplots\n",
    "plt.figure(figsize=(18, 12))\n",
    "\n",
    "# Loop through the DataFrames to generate word clouds\n",
    "for i, (df, title) in enumerate(posts_data):\n",
    "    # Filter for hateful comments with a score greater than 0\n",
    "    hateful_comments = df[df['hateful Score'] > 0]\n",
    "    \n",
    "    if hateful_comments.empty:  # Skip if there are no hateful comments\n",
    "        continue\n",
    "\n",
    "    # Create TF-IDF Vectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words='english', min_df=1, max_features=500)\n",
    "\n",
    "    # Fill NaN values and fit-transform the data\n",
    "    hateful_comments['text'] = hateful_comments['text'].fillna('')\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(hateful_comments['text'].values)\n",
    "\n",
    "    # Sum the TF-IDF scores for each term across all documents\n",
    "    tfidf_sum = tfidf_matrix.sum(axis=0)\n",
    "\n",
    "    # Get words and corresponding TF-IDF scores\n",
    "    words = tfidf_vectorizer.get_feature_names_out()\n",
    "    tfidf_scores = tfidf_sum.A1  # Convert to a 1D array\n",
    "    word_scores = dict(zip(words, tfidf_scores))\n",
    "\n",
    "    # Generate the word cloud\n",
    "    wordcloud = WordCloud(width=1500, height=800, background_color='white', colormap='viridis').generate_from_frequencies(word_scores)\n",
    "\n",
    "    # Plot the word cloud\n",
    "    plt.subplot(3, 2, i + 1)  # Arrange plots in 3 rows and 2 columns\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')  # Hide the axes\n",
    "    plt.title(title)\n",
    "\n",
    "# Adjust layout and display\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `ge2020_posts` is your DataFrame containing the toxic comments\n",
    "# Filter the DataFrame to get comments with a toxic score greater than 0\n",
    "toxic_comments = ge2020_posts[ge2020_posts['toxic Score'] > 0]\n",
    "\n",
    "# Create TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', min_df=1, max_features=500)\n",
    "\n",
    "# Fill NaN values and fit-transform the data\n",
    "toxic_comments['text'] = toxic_comments['text'].fillna('')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(toxic_comments['text'].values)\n",
    "\n",
    "# Sum the TF-IDF scores for each term across all documents\n",
    "tfidf_sum = tfidf_matrix.sum(axis=0)\n",
    "\n",
    "# Get words and corresponding TF-IDF scores\n",
    "words = tfidf_vectorizer.get_feature_names_out()\n",
    "tfidf_scores = tfidf_sum.A1  # Convert to a 1D array\n",
    "word_scores = dict(zip(words, tfidf_scores))\n",
    "\n",
    "# Generate the word cloud\n",
    "wordcloud = WordCloud(width=1500, height=800, background_color='white', colormap='viridis').generate_from_frequencies(word_scores)\n",
    "\n",
    "# Plot the word cloud\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')  # Hide the axes\n",
    "plt.title('Word Cloud for Most Toxic Comments Overall')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `ge2020_posts` is your DataFrame containing the hateful comments\n",
    "# Filter the DataFrame to get comments with a hateful score greater than 0\n",
    "hateful_comments = ge2020_posts[ge2020_posts['hateful Score'] > 0]\n",
    "\n",
    "# Create TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', min_df=1, max_features=500)\n",
    "\n",
    "# Fill NaN values and fit-transform the data\n",
    "hateful_comments['text'] = hateful_comments['text'].fillna('')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(hateful_comments['text'].values)\n",
    "\n",
    "# Sum the TF-IDF scores for each term across all documents\n",
    "tfidf_sum = tfidf_matrix.sum(axis=0)\n",
    "\n",
    "# Get words and corresponding TF-IDF scores\n",
    "words = tfidf_vectorizer.get_feature_names_out()\n",
    "tfidf_scores = tfidf_sum.A1  # Convert to a 1D array\n",
    "word_scores = dict(zip(words, tfidf_scores))\n",
    "\n",
    "# Generate the word cloud\n",
    "wordcloud = WordCloud(width=1500, height=800, background_color='white', colormap='viridis').generate_from_frequencies(word_scores)\n",
    "\n",
    "# Plot the word cloud\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')  # Hide the axes\n",
    "plt.title('Word Cloud for Most Hateful Comments Overall')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monthly Average Toxic Scores and Percentage of Comments Manually Removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by day and calculate the average score\n",
    "average_toxicity_and_moderation_per_day_df = results_df.groupby(results_df['year_month']).agg(\n",
    "    average_num_removed=('is_manual_removed', 'mean'), \n",
    "    average_toxic_score=('toxic_score', 'mean'),\n",
    "    average_hateful_score=('hateful_score', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "# create percentage column\n",
    "average_toxicity_and_moderation_per_day_df['removed_percentage'] = average_toxicity_and_moderation_per_day_df['average_num_removed']*100\n",
    "\n",
    "# Convert the 'year_month' column to string or datetime format\n",
    "average_toxicity_and_moderation_per_day_df['year_month'] = average_toxicity_and_moderation_per_day_df['year_month'].astype(str)\n",
    "\n",
    "# Convert 'year_month' column to datetime format\n",
    "average_toxicity_and_moderation_per_day_df['year_month'] = pd.to_datetime(average_toxicity_and_moderation_per_day_df['year_month'])\n",
    "\n",
    "average_toxicity_and_moderation_per_day_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the plot\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "# Plot the first DataFrame on the left y-axis with transparency\n",
    "ax1.plot(average_toxicity_and_moderation_per_day_df['year_month'], \n",
    "         average_toxicity_and_moderation_per_day_df['average_toxic_score'], \n",
    "         label='Toxic Score', color='green', alpha=0.7)\n",
    "ax1.set_xlabel('Year')\n",
    "ax1.set_ylabel('Average Toxic Score', color='green')\n",
    "ax1.tick_params(axis='y', labelcolor='green')\n",
    "\n",
    "# Rotate the tick labels more (ensure this is applied to ax1)\n",
    "plt.xticks(rotation=60)  # Rotate by 60 degrees (you can adjust as needed)\n",
    "\n",
    "# Create a second y-axis for the second variable with transparency\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(average_toxicity_and_moderation_per_day_df['year_month'], \n",
    "         average_toxicity_and_moderation_per_day_df['removed_percentage'], \n",
    "         label='Number of Comments Manually Removed (%)', color='black', alpha=0.7)\n",
    "ax2.set_ylabel('Number of Comments Manually Removed (%)', color='black')\n",
    "ax2.tick_params(axis='y', labelcolor='black')\n",
    "\n",
    "# Format x-axis to show monthly ticks and set them quarterly (every 3 months)\n",
    "ax1.xaxis.set_major_locator(mdates.MonthLocator(bymonthday=15, interval=3))  # Quarterly ticks\n",
    "ax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))  # Format as year-month\n",
    "\n",
    "# Turn off x-axis tick labels for ax2 to prevent duplication\n",
    "ax2.get_xaxis().set_visible(False)\n",
    "\n",
    "# Set title\n",
    "plt.title('Monthly Average Toxic Scores and Percentage of Comments Manually Removed')\n",
    "\n",
    "# Use tight_layout for better spacing\n",
    "fig.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toxicity Score against % Comments Manually Removed by Username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate to count the number of manually removed comments per subreddit per month\n",
    "monthly_manual_removed = results_df.groupby(['year_month', 'subreddit_name'])['is_manual_removed'].sum().reset_index()\n",
    "monthly_manual_removed.rename(columns={'is_manual_removed': 'count_manual_removed'}, inplace=True)\n",
    "\n",
    "# Plot using seaborn with the custom color palette\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.lineplot(data=monthly_manual_removed, x='year_month', y='count_manual_removed', hue='subreddit_name', \n",
    "             marker='o', palette=custom_palette)\n",
    "\n",
    "# Formatting the plot\n",
    "plt.xlabel('Year-Month')\n",
    "plt.ylabel('Count of Manually Removed Comments')\n",
    "plt.title('Monthly Count of Manually Removed Comments by Subreddit')\n",
    "plt.xticks(rotation=45)  # Rotate for readability\n",
    "plt.legend(title='Subreddit', bbox_to_anchor=(1.05, 1), loc='upper left')  # Place legend outside the plot\n",
    "# Add gridlines\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'is_manual_removed' is treated as a numeric column (if it's not already)\n",
    "results_df['is_manual_removed'] = results_df['is_manual_removed'].astype(float)\n",
    "\n",
    "# Group by 'username' and calculate the mean for 'toxic_score' and percentage for 'is_manual_removed'\n",
    "average_toxic_data_by_user = results_df.groupby('username').agg(\n",
    "    average_toxic_score=('toxic_score', 'mean'),\n",
    "    percentage_is_manual_removed=('is_manual_removed', 'mean'),\n",
    "    most_frequent_subreddit=('subreddit_name', lambda x: x.value_counts().idxmax())\n",
    ")\n",
    "\n",
    "# Convert mean to percentage\n",
    "average_toxic_data_by_user['percentage_is_manual_removed'] *= 100\n",
    "\n",
    "average_toxic_data_by_user.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = {'r/singapore': \"#4c72b0\", 'r/singaporeraw': \"#55a868\", 'r/singaporehappenings': \"#c44e52\"}\n",
    "\n",
    "# Plot the scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Map the 'most_frequent_subreddit' to colors based on the palette\n",
    "colors = average_toxic_data_by_user['most_frequent_subreddit'].map(palette)\n",
    "\n",
    "# Plot with colors based on the most frequent subreddit\n",
    "plt.scatter(\n",
    "    average_toxic_data_by_user['average_toxic_score'], \n",
    "    average_toxic_data_by_user['percentage_is_manual_removed'], \n",
    "    c=colors, \n",
    "    alpha=0.6\n",
    ")\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Average Toxic Score')\n",
    "plt.ylabel('Percentage of Comments Manually Removed')\n",
    "plt.title('Scatter Plot of Average Toxic Score vs. Percentage of Comments Manually Removed by Username')\n",
    "\n",
    "# Create legend for subreddits\n",
    "legend_elements = [Patch(facecolor=color, label=subreddit) for subreddit, color in palette.items()]\n",
    "plt.legend(handles=legend_elements, title=\"Subreddit\")\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post Count by subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by 'subreddit_id' and get the count for each subreddit\n",
    "subreddit_counts = reddit_df.groupby('subreddit_id').size().reset_index(name='count')\n",
    "\n",
    "# Display the result\n",
    "print(subreddit_counts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
