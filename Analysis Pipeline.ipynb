{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hatefulness and Toxicity Analysis\n",
    "\n",
    "This notebook is meant to create a pipeline for toxicity and hatefulness analysis, specifically for reddit data. As reddit data tends to be more toxic and hateful than other social media pages, we will focus this notebook on analysing existing Singaporean subreddit data provided in class.\n",
    "\n",
    "The aim of this notebook is to provide a formmat for users to follow to recreate the results we had, and also to follow our methodology of analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library Imports\n",
    "import ast\n",
    "import datetime\n",
    "import html\n",
    "import io\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "import base64\n",
    "import dash\n",
    "import dash_bootstrap_components as dbc\n",
    "from dash import Dash, dcc, html, dash_table, Input, Output, State, callback\n",
    "from dash.dash_table.Format import Group\n",
    "from dash.dependencies import Input, Output, State\n",
    "from dash_bootstrap_templates import load_figure_template\n",
    "\n",
    "# Gensim Imports\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "import gensim.utils as gu\n",
    "import ldamallet\n",
    "\n",
    "# Hugging Face & Transformer Imports\n",
    "from huggingface_hub import hf_hub_download\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Matplotlib Imports\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.ticker import FuncFormatter, MaxNLocator\n",
    "\n",
    "# NLTK Imports\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Plotly Imports\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.graph_objs as go  # Duplicate alias but keeping it here if both are required\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Pandas and Numpy Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Scipy and Statsmodels Imports\n",
    "from scipy.stats import f_oneway  # ANOVA test\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "# Scikit-Learn Imports\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "# Visualization Imports\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Text Analysis Imports\n",
    "import emoji\n",
    "from langdetect import detect\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Torch Imports (for models on local system or device)\n",
    "import torch\n",
    "\n",
    "# ONNX Runtime (for deploying models using ONNX)\n",
    "import onnxruntime as rt\n",
    "\n",
    "# NLTK Downloads\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the data (STORE FILES IN SUBFOLDER CALLED DATA)\n",
    "data2020_df = pd.read_csv('./data/Reddit-Threads_2020-2021.csv',  lineterminator='\\n', encoding='utf8')\n",
    "data2022_df = pd.read_csv('./data/Reddit-Threads_2022-2023.csv', lineterminator='\\n', encoding='utf8')\n",
    "print(len(data2020_df)) #2663782\n",
    "print(len(data2022_df)) #1840541"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "  pd.options.mode.copy_on_write = True\n",
    "\n",
    "  #Remove rows with empty review_text\n",
    "  df = df[df['text'].notnull()]\n",
    "\n",
    "  #Remove emoji rows\n",
    "  df['text'] = df['text'].apply(lambda x: emoji.replace_emoji(x,''))\n",
    "\n",
    "  #Remove punctuation\n",
    "  df['text'] = df['text'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "\n",
    "  #Remove all rows that has non ASCII characters\n",
    "  df = df[df['text'].apply(lambda x: all(ord(c) < 128 for c in x))]\n",
    "\n",
    "  #Set all to lower case\n",
    "  df['text'] = df['text'].str.lower()\n",
    "\n",
    "  df = df.reset_index(drop=True)\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get random sample of 500,000\n",
    "length_2020 = len(data2020_df)\n",
    "length_2022 = len(data2022_df)\n",
    "size2020 = int(500000 * (length_2020/(length_2020 + length_2022)))\n",
    "size2022 = 500000 - size2020\n",
    "\n",
    "sample_2020 = data2020_df.sample(n=size2020, random_state=42)\n",
    "sample_2022 = data2022_df.sample(n=size2022, random_state=42)\n",
    "\n",
    "combined_df = pd.concat([sample_2020, sample_2022], axis=0)\n",
    "combined_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(len(combined_df))  #500,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get a glimpse of the data\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean Data:\n",
    "cleaned_df = clean_data(combined_df)\n",
    "\n",
    "print(len(cleaned_df)) #original 439642\n",
    "print(cleaned_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save data to csv (Only need to run once)\n",
    "cleaned_df.to_csv('Reddit_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df = cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data to appropriate datatypes\n",
    "reddit_df['text'] = reddit_df['text'].astype(str)\n",
    "reddit_df['timestamp'] = pd.to_datetime(reddit_df['timestamp'])\n",
    "reddit_df['username'] = reddit_df['username'].astype(str)\n",
    "reddit_df['link'] = reddit_df['link'].astype(str)\n",
    "reddit_df['link_id'] = reddit_df['link_id'].astype(str)\n",
    "reddit_df['parent_id'] = reddit_df['parent_id'].astype(str)\n",
    "reddit_df['id'] = reddit_df['id'].astype(str)\n",
    "reddit_df['subreddit_id'] = reddit_df['subreddit_id'].astype(str)\n",
    "reddit_df['moderation\\r'] = reddit_df['moderation\\r'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove \\r from column names\n",
    "reddit_df.columns = reddit_df.columns.str.strip()\n",
    "\n",
    "# Strip \\r and other whitespace characters from a specific column (e.g., 'column_name')\n",
    "reddit_df['Topic'] = reddit_df['Topic'].str.strip()\n",
    "\n",
    "reddit_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract date from datetime stamp\n",
    "reddit_df['timestamp'] = reddit_df['timestamp'].dt.date\n",
    "reddit_df['timestamp'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations:\n",
    "\n",
    "1. Number of Comments across Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of comments per day\n",
    "num_of_comments_per_day_df = reddit_df.groupby('timestamp')['id'].count()\n",
    "\n",
    "num_of_comments_per_day_df.head()\n",
    "\n",
    "# plot number of comments against time\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(num_of_comments_per_day_df.index, num_of_comments_per_day_df.values)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Comments')\n",
    "plt.title('Number of Comments per Day')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
