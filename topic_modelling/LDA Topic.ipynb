{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "import gensim.utils as gu\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import ldamallet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    reddit_df = pd.read_csv(path, lineterminator='\\n', encoding='utf8')\n",
    "    return reddit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_gensim(text):\n",
    "    \"\"\"Tokenizes and processes the text using Gensim.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        return ' '.join(gu.simple_preprocess(text))\n",
    "    else:\n",
    "        return ''  # Return an empty string for non-string inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df):\n",
    "    df['clean_text'] = df['text'].str.lower()\n",
    "    print(\"cleaned_lower\")\n",
    "    df['clean_text'] = df['clean_text'].str.replace(r'[^a-zA-Z\\s]', ' ',regex=True) \n",
    "    df['clean_text'] = df['clean_text'].str.replace(r'\\s{2,}', ' ',regex=True)   \n",
    "    print(\"cleaned_regex\") \n",
    "    df['clean_text'] = df['clean_text'].apply(preprocess_gensim)\n",
    "    print(\"cleaned_preprocessed\")\n",
    "    df['clean_text'] = df['clean_text'].apply(word_tokenize)\n",
    "    print(\"cleaned_tokenized\")\n",
    "    df['clean_text'] = df['clean_text'].apply(lambda x:[word for word in x if word not in stopwords.words(\"english\") and word.isalpha()])\n",
    "    print(\"cleaned_stopwords\")\n",
    "    df['clean_text'] = df['clean_text'].apply(lambda x: [WordNetLemmatizer().lemmatize(word) for word in x])\n",
    "    print(\"cleaned_Lemmatized\")\n",
    "    df['clean_text'] = df['clean_text'].apply(lambda x: [word for word in x if nltk.pos_tag([word])[0][1] == 'NN'])\n",
    "    print(\"cleaned_tagged\")\n",
    "    df = df[df['clean_text'].map(lambda x: len(x)) > 1].reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionary(reddit_df):\n",
    "    texts = reddit_df['clean_text']\n",
    "    id2word = corpora.Dictionary(texts)\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "    return texts, id2word, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mallet(system,folder_path):\n",
    "    os.environ['MALLET_HOME']=folder_path\n",
    "    if system == 'windows': mallet_path = folder_path+\"\\\\bin\\\\mallet.bat\"\n",
    "    elif system == 'mac': mallet_path = folder_path+\"/bin/mallet\"\n",
    "    return mallet_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your seed topics\n",
    "seed_topics = {\n",
    "    0: \"Political\",\n",
    "    1: \"Covid-19\",\n",
    "    2: \"Race & Religion\",\n",
    "    3: \"Transport\",\n",
    "    4: \"Relationships\",\n",
    "    5: \"Crime\",\n",
    "    6: \"Housing\",\n",
    "    7: \"Education\",\n",
    "    8: \"Work\"\n",
    "}\n",
    "\n",
    "# Define the seed words for each topic\n",
    "seed_words = {\n",
    "    \"Political\": [\"ge\", \"general election\", \"affair\", \"mp\", \"politician\", \"politics\"],\n",
    "    \"Covid-19\": [\"covid-19\", \"infection\", \"vaccine\", \"lockdown\", \"circuit breaker\", \"mask\", \"cough\"],\n",
    "    \"Race & Religion\": [\"chinese\", \"malay\", \"indian\", \"angmoh\", \"culture\", \"christian\", \"buddhist\", \"muslim\", \"racist\", \"CECA\"],\n",
    "    \"Transport\": [\"breakdown\", \"train\", \"mrt\", \"lrt\", \"bus\", \"simplygo\"],\n",
    "    \"Relationships\": [\"relationships\", \"husband\", \"wife\", \"bf\", \"gf\", \"breakup\", \"cheat\", \"affair\", \"lover\", \"divorce\", \"love\"],\n",
    "    \"Crime\": [\"crime\", \"case\", \"police\", \"murder\", \"kill\", \"death\", \"scam\"],\n",
    "    \"Housing\": [\"hdb\", \"price\", \"bto\", \"resale\"],\n",
    "    \"Education\": [\"student\", \"psle\", \"study\", \"alevel\", \"olevel\", \"exam\", \"school\"],\n",
    "    \"Work\": [\"ot\", \"salary\", \"unemployed\", \"boss\", \"job\", \"laoban\", \"colleague\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the function for topic modeling\n",
    "def topic_modelling(model, corpus, texts, data, seed_topics):\n",
    "    output_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(model[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        print(f\"Document {i}, Topics: {row}\")\n",
    "\n",
    "        # If the text is empty, classify it as 'Others'\n",
    "        if not texts[i]:\n",
    "            output_df = pd.concat([\n",
    "                output_df,\n",
    "                pd.DataFrame([[10, 'Others', 1.000, '']], columns=['Topic Number', 'Topic', 'Perc_Contribution', 'Topic_Keywords'])\n",
    "            ], ignore_index=True)\n",
    "        else:\n",
    "            for j, (topic_num, prop_topic) in enumerate(row):\n",
    "                if j == 0:  # Dominant topic (highest contribution)\n",
    "                    wp = model.show_topic(topic_num)\n",
    "                    topic_keywords = \", \".join([word for word, prop in wp])\n",
    "\n",
    "                    # Map the topic number to a predefined topic using seed_topics\n",
    "                    topic_name = seed_topics.get(int(topic_num), 'Unknown')\n",
    "\n",
    "                    output_df = pd.concat([\n",
    "                        output_df,\n",
    "                        pd.DataFrame([[int(topic_num), topic_name, round(prop_topic, 4), topic_keywords]], \n",
    "                                      columns=['Topic Number', 'Topic', 'Perc_Contribution', 'Topic_Keywords'])\n",
    "                    ], ignore_index=True)\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "    # Concatenate the original data with the topic modeling results\n",
    "    output_df = pd.concat([data, output_df], axis=1)\n",
    "\n",
    "    # Remove any unnecessary columns such as 'clean_text' if needed\n",
    "    output_df = output_df.drop(['clean_text', 'Perc_Contribution', 'Topic_Keywords'], axis=1, errors='ignore')\n",
    "\n",
    "    return output_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df = load_data('./data/RedditToxicityScores.csv') #Change File Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned_lower\n",
      "cleaned_regex\n",
      "cleaned_preprocessed\n",
      "cleaned_tokenized\n",
      "cleaned_stopwords\n",
      "cleaned_Lemmatized\n",
      "cleaned_tagged\n"
     ]
    }
   ],
   "source": [
    "reddit_df_processed = preprocessing(reddit_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          [think, singaporean, dont, damn, taiwan, belong]\n",
      "1                      [fair, point, secrecy, aspect, mind]\n",
      "2         [gt, binary, think, im, blind, majority, privi...\n",
      "3                                          [boo, boo, lmao]\n",
      "4         [simple, trick, insta, wedding, need, surface,...\n",
      "                                ...                        \n",
      "347598    [gt, gon, force, purchase, try, public, call, ...\n",
      "347599                                         [need, grow]\n",
      "347600                            [bickering, adult, level]\n",
      "347601    [kid, tbh, age, theyre, life, point, need, wan...\n",
      "347602                   [lianhua, government, doubt, work]\n",
      "Name: clean_text, Length: 347603, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(reddit_df_processed['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, id2word, corpus = create_dictionary(reddit_df_processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          [think, singaporean, dont, damn, taiwan, belong]\n",
      "1                      [fair, point, secrecy, aspect, mind]\n",
      "2         [gt, binary, think, im, blind, majority, privi...\n",
      "3                                          [boo, boo, lmao]\n",
      "4         [simple, trick, insta, wedding, need, surface,...\n",
      "                                ...                        \n",
      "347598    [gt, gon, force, purchase, try, public, call, ...\n",
      "347599                                         [need, grow]\n",
      "347600                            [bickering, adult, level]\n",
      "347601    [kid, tbh, age, theyre, life, point, need, wan...\n",
      "347602                   [lianhua, government, doubt, work]\n",
      "Name: clean_text, Length: 347603, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary<89191 unique tokens: ['belong', 'damn', 'dont', 'singaporean', 'taiwan']...>\n"
     ]
    }
   ],
   "source": [
    "print(id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1)], [(6, 1), (7, 1), (8, 1), (9, 1), (10, 1)], [(5, 2), (9, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 2), (16, 1), (17, 1), (18, 1), (19, 2), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 2), (28, 3), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1)], [(46, 2), (47, 1)], [(48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 1)]]\n"
     ]
    }
   ],
   "source": [
    "print(corpus[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "mallet_path = r'C:\\Users\\rhyde\\mallet-2.0.8\\mallet-2.0.8\\bin\\mallet.bat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['MALLET_HOME'] = r'C:\\Users\\rhyde\\mallet-2.0.8\\mallet-2.0.8'\n",
    "os.environ['PATH'] = os.environ['PATH'] + os.pathsep + r'C:\\Users\\rhyde\\mallet-2.0.8\\mallet-2.0.8\\bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mallet(mallet_path, num_topics, id2word, corpus):\n",
    "    # Use Gensim's wrapper for MALLET\n",
    "    return ldamallet.LdaMallet(\n",
    "        mallet_path=mallet_path, \n",
    "        corpus=corpus, \n",
    "        num_topics=num_topics, \n",
    "        id2word=id2word\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "mallet = create_mallet(mallet_path=mallet_path, num_topics=10, id2word=id2word, corpus=corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = topic_modelling(model=mallet,corpus=corpus,texts=texts,data=reddit_df, seed_topics = seed_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_df.to_csv('topic_model_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                                                      text  \\\n",
       "0       i think most singaporeans dont give a damn who...   \n",
       "1       fair point the secrecy aspect of it slipped my...   \n",
       "2                                                   range   \n",
       "3       gt this is binary thinking because you think t...   \n",
       "4                                     boo boo poor u lmao   \n",
       "...                                                   ...   \n",
       "396293  gt what are they gonna force people to purchas...   \n",
       "396294    you did what you could he just needs to grow up   \n",
       "396295            indeed\\n\\nsame bickering on adult level   \n",
       "396296  thats all kids tbh depending on the age theyre...   \n",
       "396297  lianhua is propagated by the chinese governmen...   \n",
       "\n",
       "                  timestamp           username  \\\n",
       "0       2020-04-11 15:49:23           invigo79   \n",
       "1       2020-04-03 09:59:08  potatetoe_tractor   \n",
       "2       2020-02-15 15:07:03     CrossfittJesus   \n",
       "3       2020-06-04 07:07:39            nomad80   \n",
       "4       2020-10-31 13:52:12            pirorok   \n",
       "...                     ...                ...   \n",
       "396293  2022-08-23 04:42:04            bullno1   \n",
       "396294  2022-04-09 02:44:52    Pvt_Twinkietoes   \n",
       "396295  2022-01-11 08:46:34            burncig   \n",
       "396296  2022-07-26 10:32:05         Boogie_p0p   \n",
       "396297  2022-03-21 02:43:55       rammingfarts   \n",
       "\n",
       "                                                     link    link_id  \\\n",
       "0       /r/singapore/comments/fz7vtl/im_quite_interest...  t3_fz7vtl   \n",
       "1       /r/singapore/comments/fu3axm/government_to_tab...  t3_fu3axm   \n",
       "2       /r/singapore/comments/f4ac70/what_is_ps_defens...  t3_f4ac70   \n",
       "3        /r/singapore/comments/gw55cx/notoracism/fsu4fyd/  t3_gw55cx   \n",
       "4       /r/singapore/comments/jl6abo/rsingapore_random...  t3_jl6abo   \n",
       "...                                                   ...        ...   \n",
       "396293  /r/singapore/comments/wux9p2/section_377a_mini...  t3_wux9p2   \n",
       "396294  /r/singapore/comments/tzegvu/rsingapore_random...  t3_tzegvu   \n",
       "396295  /r/singapore/comments/s184kd/psps_leong_mun_wa...  t3_s184kd   \n",
       "396296  /r/singapore/comments/w81l82/rsingapore_random...  t3_w81l82   \n",
       "396297  /r/singapore/comments/tiw1t8/rsingapore_random...  t3_tiw1t8   \n",
       "\n",
       "         parent_id       id subreddit_id  \\\n",
       "0        t3_fz7vtl  fn3gbrg     t5_2qh8c   \n",
       "1       t1_fmasya5  fmau5k3     t5_2qh8c   \n",
       "2        t3_f4ac70  fhp05xc     t5_2qh8c   \n",
       "3       t1_fsu3dsf  fsu4fyd     t5_2qh8c   \n",
       "4       t1_gap4e9y  gap4vkl     t5_2qh8c   \n",
       "...            ...      ...          ...   \n",
       "396293  t1_ilcinzf  ilf1evi     t5_2qh8c   \n",
       "396294  t1_i3zj3bm  i3zjpnf     t5_2qh8c   \n",
       "396295  t1_hs6nhfq  hs6oc6n     t5_2qh8c   \n",
       "396296  t1_ihowm0f  ihp4ykd     t5_2qh8c   \n",
       "396297  t1_i1hd9ie  i1hghen     t5_2qh8c   \n",
       "\n",
       "                                             moderation\\r  hateful Score  \\\n",
       "0       {'removal_reason': None, 'collapsed': False, '...      -0.582897   \n",
       "1       {'removal_reason': None, 'collapsed': False, '...      -1.116736   \n",
       "2       {'removal_reason': None, 'collapsed': False, '...      -1.027191   \n",
       "3       {'removal_reason': None, 'collapsed': False, '...      -0.419287   \n",
       "4       {'removal_reason': None, 'collapsed': False, '...      -0.952112   \n",
       "...                                                   ...            ...   \n",
       "396293  {'controversiality': 0, 'collapsed_reason_code...      -0.900960   \n",
       "396294  {'controversiality': 0, 'collapsed_reason_code...      -0.993730   \n",
       "396295  {'controversiality': 0, 'collapsed_reason_code...      -1.085314   \n",
       "396296  {'controversiality': 0, 'collapsed_reason_code...      -1.055493   \n",
       "396297  {'controversiality': 0, 'collapsed_reason_code...      -0.889679   \n",
       "\n",
       "        hateful HR  toxic Score  toxic HR\\r  Topic Number          Topic  \n",
       "0                0    -0.419338           0           0.0      Political  \n",
       "1                0    -1.869363           0           4.0  Relationships  \n",
       "2                0    -0.798016           0           4.0  Relationships  \n",
       "3                0     1.119165           1           5.0          Crime  \n",
       "4                0     1.197503           1           9.0        Unknown  \n",
       "...            ...          ...         ...           ...            ...  \n",
       "396293           0    -0.473879           0           NaN            NaN  \n",
       "396294           0     0.136894           1           NaN            NaN  \n",
       "396295           0     0.021013           0           NaN            NaN  \n",
       "396296           0     0.437487           1           NaN            NaN  \n",
       "396297           0    -0.808488           0           NaN            NaN  \n",
       "\n",
       "[396298 rows x 15 columns]>"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df.head"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
